#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Citation Extractor Module for Reference Agent
ä»markdownæ–‡æœ¬ä¸­æå–å’Œè§£æå¼•ç”¨ä¿¡æ¯
"""

import re
import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass


@dataclass
class Citation:
    """å¼•ç”¨ä¿¡æ¯æ•°æ®ç±»"""
    anchor: str  # é¡µé¢é”šç‚¹ï¼Œå¦‚ "page-9-0"
    authors: List[str]  # ä½œè€…åˆ—è¡¨
    title: str  # è®ºæ–‡æ ‡é¢˜
    year: str  # å‘è¡¨å¹´ä»½
    venue: str  # å‘è¡¨åœºæ‰€/æœŸåˆŠ
    doi: str = ""  # DOI
    url: str = ""  # URL
    context: str = ""  # å¼•ç”¨ä¸Šä¸‹æ–‡
    arxiv_id: str = ""  # arXiv ID
    
    def get_cache_key(self) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_str = f"{self.authors}_{self.title}_{self.year}"
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'anchor': self.anchor,
            'authors': self.authors,
            'title': self.title,
            'year': self.year,
            'venue': self.venue,
            'doi': self.doi,
            'url': self.url,
            'context': self.context,
            'arxiv_id': self.arxiv_id
        }


class CitationExtractor:
    """å¼•ç”¨æå–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # å¼•ç”¨æ¨¡å¼åŒ¹é…
        self.citation_patterns = [
            # æ ‡å‡†æ¨¡å¼: [\\(Author,](#page-x-y) [year\\)](#page-x-y)
            r'\[\\?\([^)]+\)\]\(#[^)]+\)\s*\[\\?\([^)]+\)\]\(#[^)]+\)',
            # æ•°å­—å¼•ç”¨: [\\[18\\]](#page-x-y) æˆ– [18](#page-x-y)
            r'\[\\*\[?\d+\\*\]?\]\(#page-\d+-\d+\)',
            # å¤šå¼•ç”¨: [\\[14,](#page-x-y) [15\\]](#page-x-y)
            r'\[\\*\[?\d+,?\s*\\*\]?\]\(#page-\d+-\d+\)',
            # ç®€åŒ–æ¨¡å¼: [Author et al., year](#page-x-y)
            r'\[[^]]+\]\(#page-\d+-\d+\)',
            # å•ä¸ªå¼•ç”¨: [\\(Author, year\\)](#page-x-y)
            r'\[\\?\([^)]+\)\]\(#page-\d+-\d+\)'
        ]
        
    def extract_relevant_citations(self, 
                                 full_text: str, 
                                 target_concept: str,
                                 context_window: int = 500) -> List[Citation]:
        """
        ä»åŸæ–‡ä¸­æå–ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„å¼•ç”¨
        
        Args:
            full_text: å®Œæ•´çš„markdownæ–‡æœ¬
            target_concept: ç›®æ ‡æ¦‚å¿µ/å…³é”®è¯
            context_window: ä¸Šä¸‹æ–‡çª—å£å¤§å°
            
        Returns:
            List[Citation]: ç›¸å…³å¼•ç”¨åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹æå–ä¸ '{target_concept}' ç›¸å…³çš„å¼•ç”¨")
        
        # 1. æ‰¾åˆ°åŒ…å«ç›®æ ‡æ¦‚å¿µçš„æ®µè½
        relevant_paragraphs = self._find_concept_paragraphs(full_text, target_concept, context_window)
        self.logger.info(f"æ‰¾åˆ° {len(relevant_paragraphs)} ä¸ªç›¸å…³æ®µè½")
        
        # 2. ä»ç›¸å…³æ®µè½ä¸­æå–å¼•ç”¨
        citations = []
        for paragraph in relevant_paragraphs:
            paragraph_citations = self._extract_citations_from_text(paragraph, full_text)
            citations.extend(paragraph_citations)
        
        # 3. å»é‡å’Œæ¸…ç†
        unique_citations = self._deduplicate_citations(citations)
        self.logger.info(f"æå–åˆ° {len(unique_citations)} ä¸ªå”¯ä¸€å¼•ç”¨")
        
        return unique_citations
    
    def _find_concept_paragraphs(self, full_text: str, target_concept: str, context_window: int) -> List[str]:
        """æ‰¾åˆ°åŒ…å«ç›®æ ‡æ¦‚å¿µçš„æ®µè½ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        paragraphs = []
        
        # ç”Ÿæˆç›¸å…³æ¦‚å¿µçš„æœç´¢æ¨¡å¼
        search_patterns = self._generate_concept_patterns(target_concept)
        self.logger.info(f"ç”Ÿæˆäº† {len(search_patterns)} ä¸ªæœç´¢æ¨¡å¼: {search_patterns}")
        
        # æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
        sentences = re.split(r'[.!?]+', full_text)
        
        for i, sentence in enumerate(sentences):
            # æ£€æŸ¥å¥å­æ˜¯å¦åŒ…å«ä»»ä½•ç›¸å…³æ¨¡å¼
            for pattern in search_patterns:
                if re.search(pattern, sentence, re.IGNORECASE):
                    # æ„å»ºä¸Šä¸‹æ–‡çª—å£
                    start_idx = max(0, i - 2)  # å‰2å¥
                    end_idx = min(len(sentences), i + 3)  # å2å¥
                    
                    context = '. '.join(sentences[start_idx:end_idx])
                    if context not in paragraphs:  # é¿å…é‡å¤
                        paragraphs.append(context)
                    break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±å¤Ÿäº†
        
        return paragraphs
    
    def _generate_concept_patterns(self, target_concept: str) -> List[str]:
        """ç”Ÿæˆæ¦‚å¿µæœç´¢çš„å¤šç§æ¨¡å¼"""
        patterns = []
        
        # åŸºç¡€æ¦‚å¿µï¼ˆåŸå§‹å½¢å¼ï¼‰
        patterns.append(re.escape(target_concept))
        
        # å¤„ç†è¿å­—ç¬¦å˜ä½“
        if ' ' in target_concept:
            # "cross attention" -> "cross-attention"
            hyphenated = target_concept.replace(' ', '-')
            patterns.append(re.escape(hyphenated))
            
            # "cross attention" -> "cross_attention"  
            underscored = target_concept.replace(' ', '_')
            patterns.append(re.escape(underscored))
        
        # å¤„ç†å•è¯å˜ä½“å’Œæ‰©å±•
        concept_words = target_concept.lower().split()
        
        if len(concept_words) >= 2:
            # ç”Ÿæˆéƒ¨åˆ†åŒ¹é…æ¨¡å¼
            for word in concept_words:
                if len(word) > 3:  # åªå¤„ç†è¾ƒé•¿çš„å•è¯
                    # "attention" -> "attention mechanism", "attention layer", etc.
                    patterns.append(f"{re.escape(word)}\\s+\\w+")
                    patterns.append(f"\\w+\\s+{re.escape(word)}")
        
        # ç‰¹æ®Šæƒ…å†µå¤„ç†
        if "attention" in target_concept.lower():
            patterns.extend([
                r"attention\s+mechanism",
                r"attention\s+layer",
                r"self-attention",
                r"multi-head\s+attention",
                r"scaled\s+dot-product\s+attention"
            ])
        
        if "cross" in target_concept.lower():
            patterns.extend([
                r"cross-modal",
                r"cross-domain", 
                r"cross-attention\s+mechanism",
                r"decoupled\s+cross-attention"
            ])
        
        # æŠ€æœ¯è®ºæ–‡å¸¸è§æ¨¡å¼
        base_concept = target_concept.replace('-', '').replace('_', ' ')
        patterns.extend([
            f"{re.escape(base_concept)}\\s+approach",
            f"{re.escape(base_concept)}\\s+method",
            f"{re.escape(base_concept)}\\s+technique",
            f"{re.escape(base_concept)}\\s+strategy",
            f"propose.*{re.escape(base_concept)}",
            f"using.*{re.escape(base_concept)}",
            f"based.*{re.escape(base_concept)}"
        ])
        
        # ä¸Šä¸‹æ–‡ç›¸å…³æ‰©å±•ï¼ˆå¯¹äºattentionç›¸å…³æ¦‚å¿µï¼‰
        if any(word in target_concept.lower() for word in ['attention', 'transformer', 'neural']):
            patterns.extend([
                r"transformer",
                r"neural\s+network",
                r"encoder-decoder",
                r"sequence-to-sequence",
                r"seq2seq",
                r"bert",
                r"gpt",
                r"vaswani.*attention",
                r"attention.*all.*need"  # "Attention is All You Need"çš„å¼•ç”¨æ¨¡å¼
            ])
        
        self.logger.debug(f"ä¸º '{target_concept}' ç”Ÿæˆçš„æœç´¢æ¨¡å¼: {patterns}")
        return list(set(patterns))  # å»é‡
    
    def _extract_citations_from_text(self, text: str, full_text: str) -> List[Citation]:
        """ä»æ–‡æœ¬ä¸­æå–å¼•ç”¨"""
        citations = []
        
        for pattern in self.citation_patterns:
            matches = re.finditer(pattern, text)
            
            for match in matches:
                citation = self._parse_citation_match(match.group(), full_text)
                if citation:
                    citation.context = text[:100] + "..." if len(text) > 100 else text
                    citations.append(citation)
        
        return citations
    
    def _parse_citation_match(self, citation_text: str, full_text: str) -> Optional[Citation]:
        """è§£æå•ä¸ªå¼•ç”¨åŒ¹é…"""
        try:
            # æå–é¡µé¢é”šç‚¹
            anchor_match = re.search(r'#(page-\d+-\d+)', citation_text)
            if not anchor_match:
                return None
            
            anchor = anchor_match.group(1)
            
            # åœ¨å‚è€ƒæ–‡çŒ®éƒ¨åˆ†æŸ¥æ‰¾å®Œæ•´ä¿¡æ¯
            reference_info = self._find_reference_by_anchor(full_text, anchor)
            
            if not reference_info:
                # å¦‚æœæ‰¾ä¸åˆ°å‚è€ƒæ–‡çŒ®ï¼Œå°è¯•ä»å¼•ç”¨æ–‡æœ¬æœ¬èº«è§£æ
                reference_info = self._parse_inline_citation(citation_text)
            
            if reference_info:
                return Citation(
                    anchor=anchor,
                    authors=reference_info.get('authors', []),
                    title=reference_info.get('title', ''),
                    year=reference_info.get('year', ''),
                    venue=reference_info.get('venue', ''),
                    doi=reference_info.get('doi', ''),
                    url=reference_info.get('url', ''),
                    arxiv_id=reference_info.get('arxiv_id', '')
                )
            
        except Exception as e:
            self.logger.warning(f"è§£æå¼•ç”¨æ—¶å‡ºé”™: {e}")
            
        return None
    
    def _find_reference_by_anchor(self, full_text: str, anchor: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®é”šç‚¹åœ¨å‚è€ƒæ–‡çŒ®éƒ¨åˆ†æŸ¥æ‰¾å®Œæ•´ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®éƒ¨åˆ† - æ”¯æŒå¤šç§æ ¼å¼
            references_patterns = [
                r'# References\s*(.*?)(?=#|\Z)',  # # References
                r'## References\s*(.*?)(?=##|\Z)',  # ## References  
                r'References\s*(.*?)(?=\n#|\Z)'     # References
            ]
            
            references_text = None
            for pattern in references_patterns:
                references_match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)
                if references_match:
                    references_text = references_match.group(1)
                    break
            
            if not references_text:
                self.logger.warning("æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†")
                return None
            
            # æŸ¥æ‰¾å¯¹åº”é”šç‚¹çš„å¼•ç”¨
            anchor_pattern = rf'<span id="{anchor}"></span>(.*?)(?=<span id=|$)'
            anchor_match = re.search(anchor_pattern, references_text, re.DOTALL)
            
            if anchor_match:
                reference_text = anchor_match.group(1).strip()
                # æ¸…ç†HTMLæ ‡ç­¾
                reference_text = re.sub(r'<[^>]+>', '', reference_text)
                return self._parse_reference_text(reference_text)
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°é”šç‚¹ {anchor} å¯¹åº”çš„å‚è€ƒæ–‡çŒ®")
            
        except Exception as e:
            self.logger.warning(f"æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®æ—¶å‡ºé”™: {e}")
            
        return None
    
    def _parse_reference_text(self, reference_text: str) -> Dict[str, Any]:
        """è§£æå‚è€ƒæ–‡çŒ®æ–‡æœ¬"""
        info = {
            'authors': [],
            'title': '',
            'year': '',
            'venue': '',
            'doi': '',
            'url': '',
            'arxiv_id': ''
        }
        
        try:
            # æå–å¹´ä»½
            year_match = re.search(r'\b(19|20)\d{2}\b', reference_text)
            if year_match:
                info['year'] = year_match.group()
            
            # æå–DOI
            doi_match = re.search(r'doi:\s*([^\s,]+)', reference_text, re.IGNORECASE)
            if doi_match:
                info['doi'] = doi_match.group(1)
            
            # æå–arXiv ID
            arxiv_match = re.search(r'arxiv:(\d+\.\d+)', reference_text, re.IGNORECASE)
            if arxiv_match:
                info['arxiv_id'] = arxiv_match.group(1)
            
            # æå–URL
            url_match = re.search(r'https?://[^\s,\]]+', reference_text)
            if url_match:
                info['url'] = url_match.group()
            
            # è§£æä½œè€…å’Œæ ‡é¢˜ï¼ˆæ”¹è¿›å¤„ç†ï¼‰
            # é€šå¸¸æ ¼å¼ï¼šä½œè€…å. å¹´ä»½. [æ ‡é¢˜](URL) *æœŸåˆŠ*, ä¿¡æ¯.
            
            # æå–æ ‡é¢˜ï¼ˆåœ¨æ–¹æ‹¬å·ä¸­ï¼‰
            title_match = re.search(r'\[([^\]]+)\]', reference_text)
            if title_match:
                info['title'] = title_match.group(1)
            
            # è§£æä½œè€…ï¼ˆå¹´ä»½ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            year_pos = reference_text.find(info['year']) if info['year'] else -1
            if year_pos > 0:
                authors_part = reference_text[:year_pos].strip()
                # ç§»é™¤æœ«å°¾çš„å¹´ä»½å’Œæ ‡ç‚¹
                authors_part = re.sub(r'\.\s*$', '', authors_part)
                if authors_part:
                    info['authors'] = [authors_part]
            
            # æå–æœŸåˆŠ/ä¼šè®®ï¼ˆåœ¨*å·ä¹‹é—´ï¼‰
            venue_match = re.search(r'\*([^*]+)\*', reference_text)
            if venue_match:
                info['venue'] = venue_match.group(1)
            
        except Exception as e:
            self.logger.warning(f"è§£æå‚è€ƒæ–‡çŒ®æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        
        return info
    
    def _parse_inline_citation(self, citation_text: str) -> Optional[Dict[str, Any]]:
        """ä»å†…è”å¼•ç”¨æ–‡æœ¬ä¸­è§£æåŸºæœ¬ä¿¡æ¯"""
        info = {
            'authors': [],
            'title': '',
            'year': '',
            'venue': '',
            'doi': '',
            'url': '',
            'arxiv_id': ''
        }
        
        try:
            # æå–å¹´ä»½
            year_match = re.search(r'\b(19|20)\d{2}\b', citation_text)
            if year_match:
                info['year'] = year_match.group()
            
            # æå–ä½œè€…ï¼ˆä»æ‹¬å·ä¸­ï¼‰
            author_match = re.search(r'\[\\?\(([^)]+)\)', citation_text)
            if author_match:
                author_text = author_match.group(1)
                # ç§»é™¤å¹´ä»½
                author_text = re.sub(r'\b(19|20)\d{2}\b', '', author_text).strip(' ,')
                info['authors'] = [author_text]
            
        except Exception as e:
            self.logger.warning(f"è§£æå†…è”å¼•ç”¨æ—¶å‡ºé”™: {e}")
        
        return info if info['authors'] or info['year'] else None
    
    def _deduplicate_citations(self, citations: List[Citation]) -> List[Citation]:
        """å»é‡å¼•ç”¨åˆ—è¡¨"""
        seen = set()
        unique_citations = []
        
        for citation in citations:
            # ä½¿ç”¨é”šç‚¹ä½œä¸ºå»é‡é”®
            if citation.anchor not in seen:
                seen.add(citation.anchor)
                unique_citations.append(citation)
        
        return unique_citations


# æµ‹è¯•å‡½æ•°
def test_citation_extractor():
    """æµ‹è¯•å¼•ç”¨æå–å™¨"""
    print("ğŸ§ª æµ‹è¯•å¼•ç”¨æå–å™¨...")
    
    extractor = CitationExtractor()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_text = """
    Recent advancements in Large Language Models (LLMs) have led to the development of 
    intelligent agents [\\(OpenAI,](#page-9-0) [2024\\)](#page-9-0). These models demonstrate 
    strong reasoning capabilities [\\(Huang](#page-9-2) [et al.,](#page-9-2) [2022\\)](#page-9-2).
    
    ## References
    
    <span id="page-9-0"></span>OpenAI. 2024. [Gpt-4 technical report.](https://arxiv.org/abs/2303.08774) *Preprint*, arXiv:2303.08774.
    
    <span id="page-9-2"></span>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. [Large language models can self-improve.](https://arxiv.org/abs/2210.11610) *Preprint*, arXiv:2210.11610.
    """
    
    # æµ‹è¯•æå–åŠŸèƒ½
    citations = extractor.extract_relevant_citations(test_text, "Large Language Models")
    
    print(f"âœ… æå–åˆ° {len(citations)} ä¸ªå¼•ç”¨:")
    for i, citation in enumerate(citations, 1):
        print(f"  {i}. {citation.authors} ({citation.year})")
        print(f"     æ ‡é¢˜: {citation.title}")
        print(f"     é”šç‚¹: {citation.anchor}")
        print(f"     arXiv: {citation.arxiv_id}")
        print()
    
    return len(citations) > 0


if __name__ == "__main__":
    test_citation_extractor()
