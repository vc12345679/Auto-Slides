#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Content Extractor Module for Reference Agent
å†…å®¹æå–æ¨¡å—ï¼Œä»æ£€ç´¢åˆ°çš„æ–‡çŒ®ä¸­æå–ç›¸å…³å†…å®¹
"""

import os
import logging
import tempfile
import requests
from pathlib import Path
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

# å¯¼å…¥ç°æœ‰çš„PDFè§£ææ¨¡å—
import sys
sys.path.append('../..')
from modules.lightweight_extractor import LightweightExtractor

from .literature_searcher import PaperResult


@dataclass
class ExtractedContent:
    """æå–çš„å†…å®¹æ•°æ®ç±»"""
    paper_info: Dict[str, Any]
    relevant_sections: List[str]
    key_sentences: List[str]
    confidence_score: float
    extraction_method: str
    full_text: str = ""
    abstract: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'paper_info': self.paper_info,
            'relevant_sections': self.relevant_sections,
            'key_sentences': self.key_sentences,
            'confidence_score': self.confidence_score,
            'extraction_method': self.extraction_method,
            'full_text': self.full_text,
            'abstract': self.abstract
        }


class ContentExtractor:
    """å†…å®¹æå–å™¨"""
    
    def __init__(self, llm_interface=None):
        self.logger = logging.getLogger(__name__)
        self.llm_interface = llm_interface
        self.temp_dir = Path(tempfile.gettempdir()) / "reference_pdfs"
        self.temp_dir.mkdir(exist_ok=True)
    
    def extract_relevant_content(self, 
                               paper_result: PaperResult,
                               target_concept: str,
                               original_context: str,
                               max_sections: int = 3) -> Optional[ExtractedContent]:
        """
        ä»è®ºæ–‡ç»“æœä¸­æå–ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„å†…å®¹
        
        Args:
            paper_result: è®ºæ–‡æ£€ç´¢ç»“æœ
            target_concept: ç›®æ ‡æ¦‚å¿µ
            original_context: åŸå§‹ä¸Šä¸‹æ–‡
            max_sections: æœ€å¤§æå–æ®µè½æ•°
            
        Returns:
            ExtractedContent: æå–çš„å†…å®¹
        """
        self.logger.info(f"å¼€å§‹æå–å†…å®¹: {paper_result.title[:50]}...")
        
        try:
            # ä¼˜å…ˆå°è¯•PDFå…¨æ–‡ï¼ˆä¿¡æ¯æ›´å®Œæ•´ï¼‰
            if paper_result.has_pdf_access():
                try:
                    return self._extract_from_pdf(paper_result, target_concept, original_context, max_sections)
                except Exception as pdf_error:
                    self.logger.warning(f"PDF extraction failed, falling back to abstract: {pdf_error}")
                    # å¦‚æœPDFå¤±è´¥ï¼Œå›é€€åˆ°æ‘˜è¦
                    if paper_result.abstract:
                        return self._extract_from_abstract(paper_result, target_concept, original_context)
            elif paper_result.abstract:
                # ä½¿ç”¨æ‘˜è¦
                return self._extract_from_abstract(paper_result, target_concept, original_context)
            else:
                self.logger.warning("No content available for extraction")
                return None
                
        except Exception as e:
            self.logger.error(f"å†…å®¹æå–å¤±è´¥: {e}")
            return None
    
    def _extract_from_pdf(self, 
                         paper_result: PaperResult,
                         target_concept: str,
                         original_context: str,
                         max_sections: int) -> Optional[ExtractedContent]:
        """ä»PDFä¸­æå–å†…å®¹"""
        try:
            # ä¸‹è½½PDF
            pdf_path = self._download_pdf(paper_result.pdf_url, paper_result.paper_id)
            if not pdf_path:
                return None
            
            # ä½¿ç”¨ç°æœ‰çš„PDFè§£æå™¨ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
            abs_pdf_path = pdf_path.absolute()
            abs_output_dir = self.temp_dir.absolute()
            
            # ç¡®ä¿åœ¨æ­£ç¡®çš„å·¥ä½œç›®å½•ä¸‹è¿è¡Œ
            import os
            original_cwd = os.getcwd()
            try:
                # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•ä»¥æ­£ç¡®åŠ è½½æ¨¡å‹
                project_root = Path(__file__).parent.parent.parent
                os.chdir(project_root)
                
                extractor = LightweightExtractor(str(abs_pdf_path), output_dir=str(abs_output_dir))
                content = extractor.extract_content()
            finally:
                # æ¢å¤åŸå·¥ä½œç›®å½•
                os.chdir(original_cwd)
            
            if not content or not content.get('full_text'):
                self.logger.warning("PDFè§£æå¤±è´¥æˆ–æ— æ–‡æœ¬å†…å®¹")
                return None
            
            full_text = content['full_text']
            
            # æå–ç›¸å…³æ®µè½
            relevant_sections = self._find_relevant_sections(full_text, target_concept, max_sections)
            
            # ä½¿ç”¨LLMæå–å…³é”®å¥å­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            key_sentences = []
            if self.llm_interface and relevant_sections:
                key_sentences = self._extract_key_sentences_with_llm(
                    relevant_sections, target_concept, original_context
                )
            else:
                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                key_sentences = self._extract_key_sentences_simple(relevant_sections, target_concept)
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            confidence_score = self._calculate_relevance_score(relevant_sections, target_concept)
            
            return ExtractedContent(
                paper_info=paper_result.to_dict(),
                relevant_sections=relevant_sections,
                key_sentences=key_sentences,
                confidence_score=confidence_score,
                extraction_method="pdf_full_text",
                full_text=full_text,
                abstract=paper_result.abstract
            )
            
        except Exception as e:
            self.logger.error(f"PDFå†…å®¹æå–å¤±è´¥: {e}")
            return None
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'pdf_path' in locals() and pdf_path and pdf_path.exists():
                try:
                    pdf_path.unlink()
                except:
                    pass
    
    def _extract_from_abstract(self, 
                             paper_result: PaperResult,
                             target_concept: str,
                             original_context: str) -> Optional[ExtractedContent]:
        """ä»æ‘˜è¦ä¸­æå–å†…å®¹"""
        try:
            abstract = paper_result.abstract
            if not abstract:
                return None
            
            # æ£€æŸ¥æ‘˜è¦ä¸ç›®æ ‡æ¦‚å¿µçš„ç›¸å…³æ€§
            if target_concept.lower() not in abstract.lower():
                self.logger.info("æ‘˜è¦ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³æ€§è¾ƒä½")
                return None
            
            # åˆ†å¥å¤„ç†
            sentences = self._split_sentences(abstract)
            relevant_sentences = [s for s in sentences if target_concept.lower() in s.lower()]
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            confidence_score = len(relevant_sentences) / len(sentences) if sentences else 0
            
            return ExtractedContent(
                paper_info=paper_result.to_dict(),
                relevant_sections=[abstract],
                key_sentences=relevant_sentences,
                confidence_score=confidence_score,
                extraction_method="abstract_only",
                abstract=abstract
            )
            
        except Exception as e:
            self.logger.error(f"æ‘˜è¦å†…å®¹æå–å¤±è´¥: {e}")
            return None
    
    def _download_pdf(self, pdf_url: str, paper_id: str) -> Optional[Path]:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            self.logger.info(f"ä¸‹è½½PDF: {pdf_url}")
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å
            filename = f"{paper_id[:10]}_{hash(pdf_url) % 10000}.pdf"
            pdf_path = self.temp_dir / filename
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
            if pdf_path.exists():
                return pdf_path
            
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(pdf_url, timeout=30, stream=True)
            response.raise_for_status()
            
            with open(pdf_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            self.logger.info(f"PDFä¸‹è½½æˆåŠŸ: {pdf_path}")
            return pdf_path
            
        except Exception as e:
            self.logger.error(f"PDFä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def _find_relevant_sections(self, 
                              full_text: str, 
                              target_concept: str, 
                              max_sections: int) -> List[str]:
        """æŸ¥æ‰¾ç›¸å…³æ®µè½"""
        try:
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = self._split_paragraphs(full_text)
            
            # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æ¦‚å¿µçš„æ®µè½
            relevant_paragraphs = []
            concept_lower = target_concept.lower()
            
            for paragraph in paragraphs:
                if len(paragraph.strip()) < 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                    continue
                    
                if concept_lower in paragraph.lower():
                    # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                    score = self._calculate_paragraph_relevance(paragraph, target_concept)
                    relevant_paragraphs.append((paragraph, score))
            
            # æŒ‰ç›¸å…³æ€§æ’åºï¼Œå–å‰Nä¸ª
            relevant_paragraphs.sort(key=lambda x: x[1], reverse=True)
            
            return [p[0] for p in relevant_paragraphs[:max_sections]]
            
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç›¸å…³æ®µè½å¤±è´¥: {e}")
            return []
    
    def _split_paragraphs(self, text: str) -> List[str]:
        """åˆ†å‰²æ®µè½"""
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        paragraphs = text.split('\n\n')
        
        # æ¸…ç†å’Œè¿‡æ»¤
        cleaned_paragraphs = []
        for p in paragraphs:
            cleaned = p.strip().replace('\n', ' ')
            if len(cleaned) > 50:  # åªä¿ç•™è¶³å¤Ÿé•¿çš„æ®µè½
                cleaned_paragraphs.append(cleaned)
        
        return cleaned_paragraphs
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        import re
        
        # ç®€å•çš„å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _calculate_paragraph_relevance(self, paragraph: str, target_concept: str) -> float:
        """è®¡ç®—æ®µè½ä¸ç›®æ ‡æ¦‚å¿µçš„ç›¸å…³æ€§"""
        try:
            paragraph_lower = paragraph.lower()
            concept_lower = target_concept.lower()
            
            # åŸºç¡€åˆ†æ•°ï¼šåŒ…å«ç›®æ ‡æ¦‚å¿µ
            score = 0.0
            if concept_lower in paragraph_lower:
                score += 0.5
            
            # é¢å¤–åˆ†æ•°ï¼šæ¦‚å¿µå‡ºç°é¢‘ç‡
            concept_count = paragraph_lower.count(concept_lower)
            score += min(concept_count * 0.1, 0.3)
            
            # é¢å¤–åˆ†æ•°ï¼šæ®µè½é•¿åº¦é€‚ä¸­
            length = len(paragraph)
            if 100 <= length <= 500:
                score += 0.2
            elif 500 < length <= 1000:
                score += 0.1
            
            return score
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—æ®µè½ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0
    
    def _extract_key_sentences_with_llm(self, 
                                      sections: List[str],
                                      target_concept: str,
                                      original_context: str) -> List[str]:
        """ä½¿ç”¨LLMæå–å…³é”®å¥å­"""
        try:
            if not self.llm_interface or not sections:
                return []
            
            # æ„å»ºæç¤ºè¯
            sections_text = '\n\n'.join(sections)
            
            prompt = f"""
            ä»ä»¥ä¸‹æ–‡çŒ®æ®µè½ä¸­æå–ä¸æ¦‚å¿µ"{target_concept}"ç›¸å…³çš„å…³é”®å¥å­ã€‚
            
            åŸå§‹ä¸Šä¸‹æ–‡ï¼š
            {original_context[:200]}...
            
            æ–‡çŒ®æ®µè½ï¼š
            {sections_text[:2000]}...
            
            è¯·æå–3-5ä¸ªæœ€ç›¸å…³çš„å…³é”®å¥å­ï¼Œæ¯ä¸ªå¥å­åº”è¯¥ï¼š
            1. ç›´æ¥åŒ…å«æˆ–è§£é‡Š"{target_concept}"
            2. æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯
            3. ä¸åŸå§‹ä¸Šä¸‹æ–‡ç›¸å…³
            
            è¯·ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
            1. [ç¬¬ä¸€ä¸ªå…³é”®å¥å­]
            2. [ç¬¬äºŒä¸ªå…³é”®å¥å­]
            ...
            """
            
            # è°ƒç”¨LLMï¼ˆä½¿ç”¨ç°æœ‰çš„extractionæ–¹æ³•ï¼‰
            response = self.llm_interface.call_for_extraction(
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»æ–‡çŒ®ä¸­æå–å…³é”®ä¿¡æ¯ã€‚",
                user_prompt=prompt
            )
            
            # è§£æå“åº”
            key_sentences = []
            for line in response.split('\n'):
                line = line.strip()
                if line and (line.startswith(('1.', '2.', '3.', '4.', '5.')) or line.startswith('-')):
                    sentence = line.split('.', 1)[-1].strip() if '.' in line else line[1:].strip()
                    if sentence:
                        key_sentences.append(sentence)
            
            return key_sentences[:5]  # æœ€å¤šè¿”å›5ä¸ª
            
        except Exception as e:
            self.logger.error(f"LLMæå–å…³é”®å¥å­å¤±è´¥: {e}")
            return []
    
    def _extract_key_sentences_simple(self, sections: List[str], target_concept: str) -> List[str]:
        """ç®€å•çš„å…³é”®å¥å­æå–"""
        try:
            key_sentences = []
            concept_lower = target_concept.lower()
            
            for section in sections:
                sentences = self._split_sentences(section)
                
                for sentence in sentences:
                    if concept_lower in sentence.lower() and len(sentence) > 30:
                        key_sentences.append(sentence)
                        
                        if len(key_sentences) >= 5:  # æœ€å¤š5ä¸ª
                            break
                
                if len(key_sentences) >= 5:
                    break
            
            return key_sentences
            
        except Exception as e:
            self.logger.error(f"ç®€å•æå–å…³é”®å¥å­å¤±è´¥: {e}")
            return []
    
    def _calculate_relevance_score(self, sections: List[str], target_concept: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°"""
        try:
            if not sections:
                return 0.0
            
            total_score = 0.0
            concept_lower = target_concept.lower()
            
            for section in sections:
                section_lower = section.lower()
                
                # åŸºç¡€åˆ†æ•°ï¼šåŒ…å«æ¦‚å¿µ
                if concept_lower in section_lower:
                    total_score += 0.3
                
                # é¢å¤–åˆ†æ•°ï¼šæ¦‚å¿µå‡ºç°é¢‘ç‡
                concept_count = section_lower.count(concept_lower)
                total_score += min(concept_count * 0.1, 0.2)
                
                # é¢å¤–åˆ†æ•°ï¼šå†…å®¹è´¨é‡ï¼ˆé•¿åº¦é€‚ä¸­ï¼‰
                if 100 <= len(section) <= 1000:
                    total_score += 0.1
            
            # å½’ä¸€åŒ–åˆ°0-1åŒºé—´
            return min(total_score / len(sections), 1.0)
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¤±è´¥: {e}")
            return 0.0


# æµ‹è¯•å‡½æ•°
def test_content_extractor():
    """æµ‹è¯•å†…å®¹æå–å™¨"""
    print("ğŸ§ª æµ‹è¯•å†…å®¹æå–å™¨...")
    
    # åˆ›å»ºæµ‹è¯•ç”¨çš„PaperResult
    test_paper = PaperResult(
        paper_id="test_paper",
        title="Attention Is All You Need",
        authors=["Vaswani", "Shazeer"],
        year="2017",
        abstract="The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
        venue="NIPS",
        confidence_score=0.8,
        search_strategy="test"
    )
    
    extractor = ContentExtractor()
    
    # æµ‹è¯•ä»æ‘˜è¦æå–
    print("\nğŸ“„ æµ‹è¯•ä»æ‘˜è¦æå–å†…å®¹...")
    result = extractor.extract_relevant_content(
        test_paper, 
        "attention mechanism", 
        "æˆ‘ä»¬æ­£åœ¨ç ”ç©¶æ³¨æ„åŠ›æœºåˆ¶åœ¨ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨"
    )
    
    if result:
        print("âœ… æå–æˆåŠŸ!")
        print(f"   æå–æ–¹æ³•: {result.extraction_method}")
        print(f"   ç›¸å…³æ€§åˆ†æ•°: {result.confidence_score:.3f}")
        print(f"   ç›¸å…³æ®µè½æ•°: {len(result.relevant_sections)}")
        print(f"   å…³é”®å¥å­æ•°: {len(result.key_sentences)}")
        
        if result.key_sentences:
            print("   å…³é”®å¥å­:")
            for i, sentence in enumerate(result.key_sentences[:2], 1):
                print(f"     {i}. {sentence[:80]}...")
        
        return True
    else:
        print("âŒ æå–å¤±è´¥")
        return False


if __name__ == "__main__":
    test_content_extractor()
