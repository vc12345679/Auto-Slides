#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Content Integrator Module for Reference Agent
å†…å®¹æ•´åˆæ¨¡å—ï¼Œå°†å¤šç¯‡æ–‡çŒ®çš„å†…å®¹æ™ºèƒ½æ•´åˆç”Ÿæˆæ‰©å±•ææ–™
"""

import os
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from dotenv import load_dotenv

from .content_extractor import ExtractedContent

# Load environment variables
if os.path.exists("../../.env"):
    load_dotenv("../../.env")
elif os.path.exists("../../env.local"):
    load_dotenv("../../env.local")

# Import prompts
import sys
sys.path.append('../..')
from prompts.reference_content_integration import (
    CONTENT_INTEGRATION_SYSTEM_PROMPT,
    create_content_integration_user_prompt,
    SIMPLE_INTEGRATION_TEMPLATE
)

# å¯¼å…¥LangChainç»„ä»¶
try:
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.schema import HumanMessage, SystemMessage
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False


@dataclass
class IntegratedContent:
    """æ•´åˆåçš„å†…å®¹"""
    expanded_content: str
    source_papers: List[Dict[str, Any]]
    integration_method: str
    quality_score: float
    summary: str = ""
    key_points: List[str] = None
    
    def __post_init__(self):
        if self.key_points is None:
            self.key_points = []
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'expanded_content': self.expanded_content,
            'source_papers': self.source_papers,
            'integration_method': self.integration_method,
            'quality_score': self.quality_score,
            'summary': self.summary,
            'key_points': self.key_points
        }


class ContentIntegrator:
    """å†…å®¹æ•´åˆå™¨"""
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.3, api_key: Optional[str] = None):
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        self.temperature = temperature
        
        # å°è¯•åŠ è½½.envæ–‡ä»¶ä¸­çš„APIå¯†é’¥
        if not api_key:
            try:
                from dotenv import load_dotenv
                import os
                load_dotenv()
                api_key = os.environ.get("OPENAI_API_KEY")
            except Exception as e:
                self.logger.warning(f"Failed to load .env file: {e}")
        
        self.api_key = api_key
        
        # åˆå§‹åŒ–LangChainæ¨¡å‹
        self._init_model()
    
    def _init_model(self):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        if not LANGCHAIN_AVAILABLE:
            self.logger.warning("LangChain not available, content integration functionality disabled")
            self.llm = None
            return
        
        if not self.api_key:
            self.logger.warning("No OpenAI API key provided, content integration functionality disabled")
            self.llm = None
            return
        
        try:
            self.llm = ChatOpenAI(
                model_name=self.model_name,
                temperature=self.temperature,
                openai_api_key=self.api_key
            )
            self.logger.info(f"Content Integrator initialized with model: {self.model_name}")
        except Exception as e:
            self.logger.error(f"Failed to initialize language model: {str(e)}")
            self.llm = None
    
    def generate_expanded_content(self, 
                                original_context: str,
                                target_concept: str,
                                extracted_contents: List[ExtractedContent],
                                max_length: int = 2000) -> Optional[IntegratedContent]:
        """
        æ•´åˆå¤šç¯‡æ–‡çŒ®å†…å®¹ç”Ÿæˆæ‰©å±•ææ–™
        
        Args:
            original_context: åŸå§‹ä¸Šä¸‹æ–‡
            target_concept: ç›®æ ‡æ¦‚å¿µ
            extracted_contents: æå–çš„å†…å®¹åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            IntegratedContent: æ•´åˆåçš„å†…å®¹
        """
        self.logger.info(f"å¼€å§‹æ•´åˆ {len(extracted_contents)} ç¯‡æ–‡çŒ®çš„å†…å®¹")
        
        if not extracted_contents:
            self.logger.warning("æ²¡æœ‰å¯æ•´åˆçš„å†…å®¹")
            return None
        
        try:
            # æŒ‰ç›¸å…³æ€§æ’åº
            sorted_contents = sorted(
                extracted_contents, 
                key=lambda x: x.confidence_score, 
                reverse=True
            )
            
            # é™åˆ¶ä½¿ç”¨å‰3ç¯‡æœ€ç›¸å…³çš„æ–‡çŒ®
            top_contents = sorted_contents[:3]
            
            # ä¼˜å…ˆå°è¯•LLMæ•´åˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°ç®€å•æ•´åˆ
            if self.llm:
                try:
                    result = self._integrate_with_llm(
                        original_context, target_concept, top_contents, max_length
                    )
                    if result:
                        return result
                except Exception as e:
                    self.logger.warning(f"LLMæ•´åˆå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ•´åˆ: {e}")
            
            # ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œæ•´åˆ
            return self._integrate_simple(
                original_context, target_concept, top_contents, max_length
            )
                
        except Exception as e:
            self.logger.error(f"å†…å®¹æ•´åˆå¤±è´¥: {e}")
            return None
    
    def _integrate_with_llm(self, 
                          original_context: str,
                          target_concept: str,
                          contents: List[ExtractedContent],
                          max_length: int) -> Optional[IntegratedContent]:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ•´åˆ"""
        try:
            # æ„å»ºæ•´åˆæç¤ºè¯
            system_prompt = CONTENT_INTEGRATION_SYSTEM_PROMPT
            
            # æ„å»ºæ–‡çŒ®ä¿¡æ¯
            literature_info = []
            for i, content in enumerate(contents, 1):
                paper_info = content.paper_info
                title = paper_info.get('title', 'Unknown title')[:60]
                authors = paper_info.get('authors', ['Unknown author'])
                year = paper_info.get('year', 'Unknown year')
                
                lit_section = [
                    f"Literature {i}: {title}... ({', '.join(authors[:2])}{'et al.' if len(authors) > 2 else ''}, {year})"
                ]
                
                if content.key_sentences:
                    lit_section.append("Key content:")
                    for j, sentence in enumerate(content.key_sentences[:3], 1):
                        lit_section.append(f"  - {sentence}")
                
                literature_info.append("\n".join(lit_section))
            
            literature_text = "\n\n".join(literature_info)
            
            user_prompt = create_content_integration_user_prompt(
                original_context, target_concept, literature_text, max_length
            )
            
            # è°ƒç”¨LLM
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æå“åº”
            result = self._parse_llm_response(response_text, contents)
            
            # éªŒè¯è´¨é‡
            quality_score = self._validate_content_quality(result['expanded_content'], target_concept)
            
            return IntegratedContent(
                expanded_content=result['expanded_content'],
                source_papers=[c.paper_info for c in contents],
                integration_method="llm_intelligent",
                quality_score=quality_score,
                summary=result.get('summary', ''),
                key_points=result.get('key_points', [])
            )
            
        except Exception as e:
            self.logger.error(f"LLMæ•´åˆå¤±è´¥: {e}")
            return None
    
    def _integrate_simple(self, 
                        original_context: str,
                        target_concept: str,
                        contents: List[ExtractedContent],
                        max_length: int) -> Optional[IntegratedContent]:
        """ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œæ•´åˆ"""
        try:
            # æ”¶é›†æ‰€æœ‰å…³é”®å¥å­
            all_sentences = []
            source_papers = []
            
            for content in contents:
                # æ·»åŠ æ–‡çŒ®ä¿¡æ¯
                paper_info = content.paper_info
                source_papers.append(paper_info)
                
                # æ·»åŠ å…³é”®å¥å­
                for sentence in content.key_sentences[:2]:  # æ¯ç¯‡æ–‡çŒ®æœ€å¤š2ä¸ªå¥å­
                    all_sentences.append(sentence)
            
            # æ„å»ºæ‰©å±•å†…å®¹ï¼ˆä½¿ç”¨è‹±æ–‡æ¨¡æ¿ï¼‰
            key_points_text = "\n".join([
                f"{i}. {sentence}" for i, sentence in enumerate(all_sentences[:5], 1)
            ])
            
            expanded_content = SIMPLE_INTEGRATION_TEMPLATE.format(
                target_concept=target_concept,
                key_points=key_points_text
            )
            
            # æ§åˆ¶é•¿åº¦
            if len(expanded_content) > max_length:
                expanded_content = expanded_content[:max_length-3] + "..."
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = self._validate_content_quality(expanded_content, target_concept)
            
            return IntegratedContent(
                expanded_content=expanded_content,
                source_papers=source_papers,
                integration_method="simple_concatenation",
                quality_score=quality_score,
                summary=f"Integrated research from {len(contents)} papers about {target_concept}",
                key_points=all_sentences[:5]
            )
            
        except Exception as e:
            self.logger.error(f"ç®€å•æ•´åˆå¤±è´¥: {e}")
            return None
    

    
    def _parse_llm_response(self, response: str, contents: List[ExtractedContent]) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        try:
            result = {
                'expanded_content': '',
                'key_points': [],
                'summary': ''
            }
            
            lines = response.split('\n')
            current_section = None
            content_lines = []
            
            for line in lines:
                line = line.strip()
                
                if line.startswith('# æ‰©å±•å†…å®¹'):
                    current_section = 'content'
                    content_lines = []
                elif line.startswith('# å…³é”®è¦ç‚¹'):
                    if current_section == 'content':
                        result['expanded_content'] = '\n'.join(content_lines).strip()
                    current_section = 'points'
                elif line.startswith('# å†…å®¹æ€»ç»“'):
                    current_section = 'summary'
                elif line:
                    if current_section == 'content':
                        content_lines.append(line)
                    elif current_section == 'points':
                        if line.startswith(('1.', '2.', '3.', '4.', '5.', '-')):
                            point = line.split('.', 1)[-1].strip() if '.' in line else line[1:].strip()
                            if point:
                                result['key_points'].append(point)
                    elif current_section == 'summary':
                        result['summary'] = line
                        break
            
            # å¦‚æœæ²¡æœ‰æ­£ç¡®è§£æåˆ°æ‰©å±•å†…å®¹ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”
            if not result['expanded_content']:
                result['expanded_content'] = response.strip()
            
            return result
            
        except Exception as e:
            self.logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return {
                'expanded_content': response.strip(),
                'key_points': [],
                'summary': 'å†…å®¹æ•´åˆå®Œæˆ'
            }
    
    def _validate_content_quality(self, content: str, target_concept: str) -> float:
        """éªŒè¯å†…å®¹è´¨é‡"""
        try:
            score = 0.0
            
            if not content:
                return 0.0
            
            # é•¿åº¦åˆç†æ€§
            length = len(content)
            if 200 <= length <= 2000:
                score += 0.3
            elif 100 <= length < 200 or 2000 < length <= 3000:
                score += 0.2
            elif length < 100:
                score += 0.1
            
            # åŒ…å«ç›®æ ‡æ¦‚å¿µ
            if target_concept.lower() in content.lower():
                score += 0.3
            
            # ç»“æ„å®Œæ•´æ€§ï¼ˆåŒ…å«å¤šä¸ªå¥å­ï¼‰
            sentences = content.split('.')
            if len(sentences) >= 3:
                score += 0.2
            elif len(sentences) >= 2:
                score += 0.1
            
            # å­¦æœ¯æ€§è¯æ±‡
            academic_words = ['ç ”ç©¶', 'è¡¨æ˜', 'å‘ç°', 'åˆ†æ', 'ç»“æœ', 'æ–¹æ³•', 'ç†è®º', 'æ¨¡å‹', 'å®éªŒ']
            academic_count = sum(1 for word in academic_words if word in content)
            score += min(academic_count * 0.05, 0.2)
            
            return min(score, 1.0)
            
        except Exception as e:
            self.logger.error(f"è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰è´¨é‡


# æµ‹è¯•å‡½æ•°
def test_content_integrator():
    """æµ‹è¯•å†…å®¹æ•´åˆå™¨"""
    print("ğŸ§ª æµ‹è¯•å†…å®¹æ•´åˆå™¨...")
    
    # åˆ›å»ºæµ‹è¯•ç”¨çš„æå–å†…å®¹
    test_contents = [
        ExtractedContent(
            paper_info={
                'title': 'Attention Is All You Need',
                'authors': ['Vaswani', 'Shazeer'],
                'year': '2017'
            },
            relevant_sections=['Transformeræ¶æ„å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶'],
            key_sentences=[
                'Transformerä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†åºåˆ—',
                'è‡ªæ³¨æ„åŠ›å…è®¸æ¨¡å‹å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®'
            ],
            confidence_score=0.9,
            extraction_method='test'
        ),
        ExtractedContent(
            paper_info={
                'title': 'BERT: Pre-training of Deep Bidirectional Transformers',
                'authors': ['Devlin', 'Chang'],
                'year': '2019'
            },
            relevant_sections=['BERTä½¿ç”¨åŒå‘Transformerç¼–ç å™¨'],
            key_sentences=[
                'BERTé€šè¿‡æ©ç è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒTransformer',
                'åŒå‘æ³¨æ„åŠ›æœºåˆ¶æé«˜äº†è¯­è¨€ç†è§£èƒ½åŠ›'
            ],
            confidence_score=0.8,
            extraction_method='test'
        )
    ]
    
    integrator = ContentIntegrator()
    
    # æµ‹è¯•ç®€å•æ•´åˆï¼ˆä¸ä¾èµ–LLMï¼‰
    print("\nğŸ“ æµ‹è¯•ç®€å•å†…å®¹æ•´åˆ...")
    result = integrator.generate_expanded_content(
        original_context="æˆ‘ä»¬æ­£åœ¨ç ”ç©¶Transformeræ¶æ„ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†",
        target_concept="æ³¨æ„åŠ›æœºåˆ¶",
        extracted_contents=test_contents
    )
    
    if result:
        print("âœ… æ•´åˆæˆåŠŸ!")
        print(f"   æ•´åˆæ–¹æ³•: {result.integration_method}")
        print(f"   è´¨é‡åˆ†æ•°: {result.quality_score:.3f}")
        print(f"   æºæ–‡çŒ®æ•°: {len(result.source_papers)}")
        print(f"   å…³é”®è¦ç‚¹æ•°: {len(result.key_points)}")
        print(f"   å†…å®¹é•¿åº¦: {len(result.expanded_content)}")
        print("\nğŸ“„ æ‰©å±•å†…å®¹é¢„è§ˆ:")
        print(result.expanded_content[:200] + "..." if len(result.expanded_content) > 200 else result.expanded_content)
        
        if result.key_points:
            print("\nğŸ”‘ å…³é”®è¦ç‚¹:")
            for i, point in enumerate(result.key_points[:3], 1):
                print(f"   {i}. {point}")
        
        return True
    else:
        print("âŒ æ•´åˆå¤±è´¥")
        return False


if __name__ == "__main__":
    test_content_integrator()
