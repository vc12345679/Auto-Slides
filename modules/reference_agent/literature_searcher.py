#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Literature Searcher Module for Reference Agent
æ–‡çŒ®æ£€ç´¢æ¨¡å—ï¼Œé›†æˆSemantic Scholarå’ŒarXiv API
"""

import re
import time
import logging
import hashlib
import requests
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from .citation_extractor import Citation


@dataclass 
class PaperResult:
    """è®ºæ–‡æ£€ç´¢ç»“æœ"""
    paper_id: str = ""
    title: str = ""
    authors: List[str] = None
    year: str = ""
    abstract: str = ""
    venue: str = ""
    doi: str = ""
    url: str = ""
    arxiv_id: str = ""
    pdf_url: str = ""
    confidence_score: float = 0.0
    search_strategy: str = ""
    full_text: str = ""
    
    def __post_init__(self):
        if self.authors is None:
            self.authors = []
    
    def has_pdf_access(self) -> bool:
        """æ˜¯å¦æœ‰PDFå…¨æ–‡è®¿é—®æƒé™"""
        return bool(self.pdf_url)
    
    def has_full_text(self) -> bool:
        """æ˜¯å¦æœ‰å…¨æ–‡å†…å®¹"""
        return bool(self.full_text)
    
    def is_valid(self) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ"""
        return bool(self.title and (self.authors or self.year))
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'paper_id': self.paper_id,
            'title': self.title,
            'authors': self.authors,
            'year': self.year,
            'abstract': self.abstract,
            'venue': self.venue,
            'doi': self.doi,
            'url': self.url,
            'arxiv_id': self.arxiv_id,
            'pdf_url': self.pdf_url,
            'confidence_score': self.confidence_score,
            'search_strategy': self.search_strategy
        }


class RateLimiter:
    """APIé€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, calls: int, period: int):
        self.calls = calls
        self.period = period
        self.call_times = []
    
    def __enter__(self):
        now = time.time()
        # ç§»é™¤è¿‡æœŸçš„è°ƒç”¨è®°å½•
        self.call_times = [t for t in self.call_times if now - t < self.period]
        
        # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾…
        if len(self.call_times) >= self.calls:
            sleep_time = self.period - (now - self.call_times[0]) + 1
            if sleep_time > 0:
                time.sleep(sleep_time)
                now = time.time()
                self.call_times = [t for t in self.call_times if now - t < self.period]
        
        self.call_times.append(now)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        pass


class LiteratureCache:
    """æ–‡çŒ®ç¼“å­˜ç®¡ç†"""
    
    def __init__(self, cache_dir: str = "literature_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.logger = logging.getLogger(__name__)
    
    def get_cache_key(self, citation: Citation) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_str = f"{citation.authors}_{citation.title}_{citation.year}"
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, cache_key: str) -> Optional[PaperResult]:
        """è·å–ç¼“å­˜çš„ç»“æœ"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.json"
            if cache_file.exists():
                import json
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return PaperResult(**data)
        except Exception as e:
            self.logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        return None
    
    def store(self, cache_key: str, result: PaperResult):
        """å­˜å‚¨ç»“æœåˆ°ç¼“å­˜"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.json"
            import json
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.warning(f"å­˜å‚¨ç¼“å­˜å¤±è´¥: {e}")


class SemanticScholarSearcher:
    """Semantic Scholar APIæœç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.rate_limiter = RateLimiter(calls=100, period=300)  # 100 calls per 5 min
        self.logger = logging.getLogger(__name__)
        
        # è¯·æ±‚å‚æ•°é…ç½®ï¼ˆç§»é™¤ä¸æ”¯æŒçš„doiå­—æ®µï¼‰
        self.search_fields = [
            'title', 'authors', 'year', 'abstract', 'venue', 
            'url', 'openAccessPdf', 'externalIds'
        ]
    
    def search(self, citation: Citation) -> Optional[PaperResult]:
        """æœç´¢è®ºæ–‡"""
        try:
            with self.rate_limiter:
                # æ„å»ºæŸ¥è¯¢
                query = self._build_query(citation)
                
                # å‘é€è¯·æ±‚
                response = requests.get(
                    f"{self.base_url}/paper/search",
                    params=query,
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    papers = data.get('data', [])
                    
                    if papers:
                        # æ‰¾åˆ°æœ€ä½³åŒ¹é…
                        best_match = self._find_best_match(papers, citation)
                        if best_match:
                            return self._create_paper_result(best_match, citation)
                else:
                    self.logger.warning(f"Semantic Scholar APIé”™è¯¯: {response.status_code}, å“åº”: {response.text[:200]}")
                    
        except Exception as e:
            self.logger.error(f"Semantic Scholaræœç´¢å¤±è´¥: {e}")
        
        return None
    
    def _build_query(self, citation: Citation) -> Dict[str, Any]:
        """æ„å»ºæœç´¢æŸ¥è¯¢"""
        query = {
            'fields': ','.join(self.search_fields),
            'limit': 10
        }
        
        # ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜æœç´¢
        if citation.title and len(citation.title) > 10:
            query['query'] = citation.title
        elif citation.authors and citation.year:
            # ä½¿ç”¨ä½œè€…å’Œå¹´ä»½
            author_name = citation.authors[0].split(',')[0] if citation.authors else ""
            query['query'] = f"{author_name} {citation.year}"
        else:
            # æœ€åä½¿ç”¨ä½œè€…å
            query['query'] = citation.authors[0] if citation.authors else ""
        
        return query
    
    def _find_best_match(self, papers: List[Dict], citation: Citation) -> Optional[Dict]:
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„è®ºæ–‡"""
        best_score = 0
        best_paper = None
        
        for paper in papers:
            score = self._calculate_match_score(paper, citation)
            if score > best_score:
                best_score = score
                best_paper = paper
        
        # åªè¿”å›ç½®ä¿¡åº¦è¶³å¤Ÿé«˜çš„ç»“æœ
        return best_paper if best_score > 0.3 else None
    
    def _calculate_match_score(self, paper: Dict, citation: Citation) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        score = 0.0
        
        # æ ‡é¢˜ç›¸ä¼¼åº¦
        if citation.title and paper.get('title'):
            title_sim = self._string_similarity(citation.title.lower(), paper['title'].lower())
            score += title_sim * 0.5
        
        # å¹´ä»½åŒ¹é…
        if citation.year and paper.get('year'):
            if str(citation.year) == str(paper['year']):
                score += 0.3
        
        # ä½œè€…åŒ¹é…
        if citation.authors and paper.get('authors'):
            author_sim = self._author_similarity(citation.authors, paper['authors'])
            score += author_sim * 0.2
        
        return score
    
    def _string_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not s1 or not s2:
            return 0.0
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œåˆ†è¯
        words1 = set(re.findall(r'\w+', s1.lower()))
        words2 = set(re.findall(r'\w+', s2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _author_similarity(self, authors1: List[str], authors2: List[Dict]) -> float:
        """è®¡ç®—ä½œè€…ç›¸ä¼¼åº¦"""
        if not authors1 or not authors2:
            return 0.0
        
        # æå–å§“æ°
        surnames1 = set()
        for author in authors1:
            # ç®€å•æå–ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªå•è¯ä½œä¸ºå§“å
            parts = author.split()
            if parts:
                surnames1.add(parts[-1].lower())  # å§“æ°é€šå¸¸åœ¨æœ€å
        
        surnames2 = set()
        for author_dict in authors2:
            name = author_dict.get('name', '')
            parts = name.split()
            if parts:
                surnames2.add(parts[-1].lower())
        
        if not surnames1 or not surnames2:
            return 0.0
        
        intersection = len(surnames1 & surnames2)
        return intersection / min(len(surnames1), len(surnames2))
    
    def _create_paper_result(self, paper: Dict, citation: Citation) -> PaperResult:
        """åˆ›å»ºè®ºæ–‡ç»“æœå¯¹è±¡"""
        # æå–ä½œè€…å
        authors = []
        for author in paper.get('authors', []):
            authors.append(author.get('name', ''))
        
        # æå–PDF URL
        pdf_url = ""
        open_access = paper.get('openAccessPdf')
        if open_access and open_access.get('url'):
            pdf_url = open_access['url']
        
        # æå–arXiv ID
        arxiv_id = ""
        external_ids = paper.get('externalIds', {})
        if external_ids and external_ids.get('ArXiv'):
            arxiv_id = external_ids['ArXiv']
        
        return PaperResult(
            paper_id=paper.get('paperId', ''),
            title=paper.get('title', ''),
            authors=authors,
            year=str(paper.get('year', '')),
            abstract=paper.get('abstract', ''),
            venue=paper.get('venue', ''),
            doi=paper.get('doi', ''),
            url=paper.get('url', ''),
            arxiv_id=arxiv_id,
            pdf_url=pdf_url,
            confidence_score=self._calculate_match_score(paper, citation),
            search_strategy="semantic_scholar"
        )


class ArXivSearcher:
    """arXiv APIæœç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "http://export.arxiv.org/api/query"
        self.rate_limiter = RateLimiter(calls=30, period=60)  # 30 calls per minute
        self.logger = logging.getLogger(__name__)
    
    def search(self, citation: Citation) -> Optional[PaperResult]:
        """æœç´¢arXivè®ºæ–‡"""
        try:
            with self.rate_limiter:
                # å¦‚æœå·²æœ‰arXiv IDï¼Œç›´æ¥æŸ¥è¯¢
                if citation.arxiv_id:
                    return self._search_by_arxiv_id(citation.arxiv_id)
                
                # å¦åˆ™ä½¿ç”¨æ ‡é¢˜æˆ–ä½œè€…æœç´¢
                query = self._build_arxiv_query(citation)
                
                response = requests.get(
                    self.base_url,
                    params=query,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = self._parse_arxiv_response(response.text, citation)
                    return result
                else:
                    self.logger.warning(f"arXiv APIé”™è¯¯: {response.status_code}")
                    
        except Exception as e:
            self.logger.error(f"arXivæœç´¢å¤±è´¥: {e}")
        
        return None
    
    def _search_by_arxiv_id(self, arxiv_id: str) -> Optional[PaperResult]:
        """æ ¹æ®arXiv IDæœç´¢"""
        query = {
            'id_list': arxiv_id,
            'max_results': 1
        }
        
        try:
            response = requests.get(self.base_url, params=query, timeout=30)
            if response.status_code == 200:
                result = self._parse_arxiv_response(response.text)
                # ç›´æ¥é€šè¿‡IDæ‰¾åˆ°çš„ç»“æœåº”è¯¥æœ‰é«˜ç½®ä¿¡åº¦
                if result:
                    result.confidence_score = 0.9  # ç›´æ¥IDåŒ¹é…ç»™é«˜ç½®ä¿¡åº¦
                return result
        except Exception as e:
            self.logger.error(f"arXiv IDæœç´¢å¤±è´¥: {e}")
        
        return None
    
    def _build_arxiv_query(self, citation: Citation) -> Dict[str, Any]:
        """æ„å»ºarXivæŸ¥è¯¢"""
        query = {
            'max_results': 10,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        # æ„å»ºæœç´¢å­—ç¬¦ä¸²
        search_terms = []
        
        if citation.title and len(citation.title) > 10:
            # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
            clean_title = re.sub(r'[^\w\s]', ' ', citation.title)
            search_terms.append(f'ti:"{clean_title}"')
        
        if citation.authors:
            author = citation.authors[0].split(',')[0].strip()
            search_terms.append(f'au:"{author}"')
        
        if search_terms:
            query['search_query'] = ' AND '.join(search_terms)
        else:
            return None
        
        return query
    
    def _parse_arxiv_response(self, xml_text: str, citation: Citation = None) -> Optional[PaperResult]:
        """è§£æarXiv XMLå“åº”"""
        try:
            import xml.etree.ElementTree as ET
            
            root = ET.fromstring(xml_text)
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªentry
            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                result = self._parse_arxiv_entry(entry)
                
                # å¦‚æœæä¾›äº†å¼•ç”¨ä¿¡æ¯ï¼Œè®¡ç®—åŒ¹é…åˆ†æ•°
                if citation:
                    score = self._calculate_arxiv_match_score(result, citation)
                    result.confidence_score = score
                    
                    # åªè¿”å›ç½®ä¿¡åº¦è¶³å¤Ÿçš„ç»“æœ
                    if score > 0.3:
                        return result
                else:
                    return result
            
        except Exception as e:
            self.logger.error(f"è§£æarXivå“åº”å¤±è´¥: {e}")
        
        return None
    
    def _parse_arxiv_entry(self, entry) -> PaperResult:
        """è§£æå•ä¸ªarXivæ¡ç›®"""
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        # æå–åŸºæœ¬ä¿¡æ¯
        title = entry.find('atom:title', ns)
        title = title.text.strip() if title is not None else ""
        
        abstract = entry.find('atom:summary', ns)
        abstract = abstract.text.strip() if abstract is not None else ""
        
        published = entry.find('atom:published', ns)
        year = ""
        if published is not None:
            year = published.text[:4]  # æå–å¹´ä»½
        
        # æå–arXiv IDå’ŒPDF URL
        arxiv_id = ""
        pdf_url = ""
        
        for link in entry.findall('atom:link', ns):
            href = link.get('href', '')
            if 'arxiv.org/abs/' in href:
                arxiv_id = href.split('/')[-1]
            elif 'arxiv.org/pdf/' in href:
                pdf_url = href
        
        # æå–ä½œè€…
        authors = []
        for author in entry.findall('atom:author', ns):
            name = author.find('atom:name', ns)
            if name is not None:
                authors.append(name.text.strip())
        
        return PaperResult(
            paper_id=arxiv_id,
            title=title,
            authors=authors,
            year=year,
            abstract=abstract,
            venue="arXiv",
            arxiv_id=arxiv_id,
            pdf_url=pdf_url,
            search_strategy="arxiv"
        )
    
    def _calculate_arxiv_match_score(self, result: PaperResult, citation: Citation) -> float:
        """è®¡ç®—arXivç»“æœçš„åŒ¹é…åˆ†æ•°"""
        score = 0.0
        
        # æ ‡é¢˜ç›¸ä¼¼åº¦
        if citation.title and result.title:
            title_sim = self._string_similarity(citation.title.lower(), result.title.lower())
            score += title_sim * 0.6
        
        # å¹´ä»½åŒ¹é…
        if citation.year and result.year:
            if str(citation.year) == str(result.year):
                score += 0.3
        
        # ä½œè€…åŒ¹é…
        if citation.authors and result.authors:
            author_sim = self._author_similarity(citation.authors, result.authors)
            score += author_sim * 0.1
        
        return score
    
    def _string_similarity(self, s1: str, s2: str) -> float:
        """å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—"""
        if not s1 or not s2:
            return 0.0
        
        words1 = set(re.findall(r'\w+', s1.lower()))
        words2 = set(re.findall(r'\w+', s2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _author_similarity(self, authors1: List[str], authors2: List[str]) -> float:
        """ä½œè€…ç›¸ä¼¼åº¦è®¡ç®—"""
        if not authors1 or not authors2:
            return 0.0
        
        surnames1 = set()
        for author in authors1:
            parts = author.split()
            if parts:
                surnames1.add(parts[-1].lower())
        
        surnames2 = set()
        for author in authors2:
            parts = author.split()
            if parts:
                surnames2.add(parts[-1].lower())
        
        if not surnames1 or not surnames2:
            return 0.0
        
        intersection = len(surnames1 & surnames2)
        return intersection / min(len(surnames1), len(surnames2))


class LiteratureSearcher:
    """æ–‡çŒ®æ£€ç´¢å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.searchers = [
            SemanticScholarSearcher(),
            ArXivSearcher()
        ]
        self.cache = LiteratureCache()
        self.logger = logging.getLogger(__name__)
    
    def search_paper(self, citation: Citation) -> Optional[PaperResult]:
        """æœç´¢è®ºæ–‡ï¼Œä½¿ç”¨çº§è”ç­–ç•¥"""
        self.logger.info(f"æœç´¢è®ºæ–‡: {citation.title} ({citation.year})")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self.cache.get_cache_key(citation)
        cached_result = self.cache.get(cache_key)
        if cached_result:
            self.logger.info("ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return cached_result
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•æœç´¢
        best_result = None
        best_score = -1  # æ”¹ä¸º-1ï¼Œè¿™æ ·0.0çš„ç»“æœä¹Ÿèƒ½è¢«é€‰ä¸­
        
        for searcher in self.searchers:
            try:
                result = searcher.search(citation)
                if result and result.is_valid():
                    # å¯¹äºæœ‰æ•ˆç»“æœï¼Œå¦‚æœç½®ä¿¡åº¦ä¸º0ï¼Œç»™äºˆåŸºç¡€åˆ†æ•°
                    if result.confidence_score == 0.0:
                        result.confidence_score = 0.1  # ç»™äºˆåŸºç¡€åˆ†æ•°
                    
                    self.logger.info(f"{searcher.__class__.__name__} æ‰¾åˆ°ç»“æœï¼Œç½®ä¿¡åº¦: {result.confidence_score:.3f}")
                    
                    if result.confidence_score > best_score:
                        best_score = result.confidence_score
                        best_result = result
                    
                    # å¦‚æœæ‰¾åˆ°é«˜ç½®ä¿¡åº¦ç»“æœï¼Œç›´æ¥è¿”å›
                    if result.confidence_score > 0.8:
                        break
                        
            except Exception as e:
                self.logger.warning(f"{searcher.__class__.__name__} æœç´¢å¤±è´¥: {e}")
        
        # ç¼“å­˜æœ€ä½³ç»“æœ
        if best_result:
            self.cache.store(cache_key, best_result)
            self.logger.info(f"æœ€ç»ˆé€‰æ‹©: {best_result.search_strategy}, ç½®ä¿¡åº¦: {best_result.confidence_score:.3f}")
        
        return best_result
    
    def search_multiple_papers(self, citations: List[Citation], max_results: int = 5) -> List[PaperResult]:
        """æ‰¹é‡æœç´¢å¤šç¯‡è®ºæ–‡"""
        results = []
        
        for citation in citations[:max_results]:
            result = self.search_paper(citation)
            if result:
                results.append(result)
            
            # é¿å…APIé™åˆ¶ï¼Œæ·»åŠ çŸ­æš‚å»¶è¿Ÿ
            time.sleep(0.5)
        
        return results


# æµ‹è¯•å‡½æ•°
def test_literature_searcher():
    """æµ‹è¯•æ–‡çŒ®æ£€ç´¢å™¨"""
    print("ğŸ” æµ‹è¯•æ–‡çŒ®æ£€ç´¢å™¨...")
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæµ‹è¯•å¼•ç”¨
    test_citation = Citation(
        anchor="test",
        authors=["OpenAI"],
        title="GPT-4 Technical Report",
        year="2024",
        venue="arXiv",
        arxiv_id="2303.08774"
    )
    
    # å…ˆæµ‹è¯•arXivæœç´¢å™¨ï¼ˆæ›´ç®€å•ï¼‰
    print("\nğŸ§ª æµ‹è¯•arXivæœç´¢å™¨...")
    arxiv_searcher = ArXivSearcher()
    arxiv_result = arxiv_searcher.search(test_citation)
    
    if arxiv_result:
        print("âœ… arXivæœç´¢æˆåŠŸ!")
        print(f"   æ ‡é¢˜: {arxiv_result.title}")
        print(f"   ä½œè€…: {', '.join(arxiv_result.authors[:2])}...")
        print(f"   å¹´ä»½: {arxiv_result.year}")
        print(f"   arXiv ID: {arxiv_result.arxiv_id}")
        print(f"   æœ‰PDF: {'æ˜¯' if arxiv_result.has_pdf_access() else 'å¦'}")
    else:
        print("âŒ arXivæœç´¢å¤±è´¥")
    
    # å†æµ‹è¯•Semantic Scholar
    print("\nğŸ§ª æµ‹è¯•Semantic Scholaræœç´¢å™¨...")
    ss_searcher = SemanticScholarSearcher()
    
    # åˆ›å»ºä¸€ä¸ªæ›´ç®€å•çš„æµ‹è¯•
    simple_citation = Citation(
        anchor="test2",
        authors=["Wei"],
        title="Attention Is All You Need",
        year="2017",
        venue="NIPS"
    )
    
    ss_result = ss_searcher.search(simple_citation)
    
    if ss_result:
        print("âœ… Semantic Scholaræœç´¢æˆåŠŸ!")
        print(f"   æ ‡é¢˜: {ss_result.title}")
        print(f"   ä½œè€…: {', '.join(ss_result.authors[:2])}...")
        print(f"   å¹´ä»½: {ss_result.year}")
        print(f"   æœ‰PDF: {'æ˜¯' if ss_result.has_pdf_access() else 'å¦'}")
        return True
    else:
        print("âŒ Semantic Scholaræœç´¢å¤±è´¥")
        
    return arxiv_result is not None


if __name__ == "__main__":
    test_literature_searcher()
